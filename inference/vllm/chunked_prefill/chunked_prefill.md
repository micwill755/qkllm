# Chunked Prefill Guide

## What is Chunked Prefill?

**Chunked prefill** is an optimization technique that breaks down the initial prompt processing (prefill phase) into smaller chunks rather than processing the entire prompt at once. The key insight is that this enables **request interleaving** - allowing small requests to be served immediately while large requests are processed gradually.

## The Core Problem

```
Traditional LLM Serving - The "Monopoly" Problem:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request Queue:                                              â”‚
â”‚ 1. "Analyze this 50-page document..." (12,000 tokens)      â”‚
â”‚ 2. "What is 2+2?" (8 tokens)                               â”‚
â”‚ 3. "Hello" (3 tokens)                                      â”‚
â”‚ 4. "Translate: Bonjour" (12 tokens)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Processing Timeline:
[â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Process 12K tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€] [8 tok] [3 tok] [12 tok]
     ^â”€â”€â”€â”€â”€â”€ 8 seconds â”€â”€â”€â”€â”€â”€^              ^0.1s^  ^0.1s^  ^0.1s^

Result: Small requests wait 8+ seconds! 
```

## The Chunked Prefill Solution

```
Chunked Prefill - The "Fair Sharing" Solution:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Same Request Queue - But Now Processed Differently:        â”‚
â”‚ 1. Large request split into chunks: [chunk1][chunk2]...    â”‚
â”‚ 2-4. Small requests processed immediately                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Processing Timeline:
[chunk1][8tok][chunk2][3tok][chunk3][12tok][chunk4][chunk5]...
  ^0.5s^ ^0.1s^ ^0.5s^ ^0.1s^ ^0.5s^ ^0.1s^  ^0.5s^  ^0.5s^

Result: Small requests get served immediately! 
```

## Visual Overview: Traditional vs Chunked

```
Traditional Prefill:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: "The quick brown fox jumps over the lazy dog..."     â”‚
â”‚ Process: [â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entire sequence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€] â†’ token   â”‚
â”‚ Memory:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (HUGE peak!)                 â”‚
â”‚ Other requests: â³ WAITING... â³                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Chunked Prefill:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: Same sequence                                        â”‚
â”‚ Chunk 1: [The quick brown fox] â†’ partial KV cache          â”‚
â”‚ Memory:  â–ˆâ–ˆâ–ˆâ–ˆ (manageable)                                  â”‚
â”‚ Other requests: âœ… SERVED! âœ…                               â”‚
â”‚ Chunk 2: [jumps over the lazy] â†’ update KV cache           â”‚
â”‚ Memory:  â–ˆâ–ˆâ–ˆâ–ˆ (manageable)                                  â”‚
â”‚ Other requests: âœ… SERVED! âœ…                               â”‚
â”‚ ... continues until complete                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## The Three Key Benefits

### 1. Memory Efficiency - Taming the O(nÂ²) Beast

```
Attention Memory Usage:

Traditional (8K sequence):
Memory â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â† 8.6GB peak (OOM!)
       â”‚    â–ˆâ–ˆ              â–ˆâ–ˆ
       â”‚   â–ˆâ–ˆ                â–ˆâ–ˆ
       â”‚  â–ˆâ–ˆ                  â–ˆâ–ˆ
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
          ^â”€â”€â”€â”€ prefillâ”€â”€â”€â”€^

Chunked (512 token chunks):
Memory â”‚ â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â† 33MB peaks (manageable)
       â”‚â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ
       â”‚                        
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
         ^chunk^ ^chunk^ ^chunk^

Memory Reduction: 99.6% lower peak usage!
```

**Why this matters:**
- **Attention computation is O(sequence_lengthÂ²)**
- 8K tokens = 64M attention elements
- 512 tokens = 262K attention elements (256x smaller!)
- Prevents out-of-memory errors on long sequences

### 2. Request Interleaving - The Fairness Revolution

```
Batch Composition Strategy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Smart Scheduler Creates Mixed Batches:                      â”‚
â”‚                                                             â”‚
â”‚ Batch 1: [decode_req1] [decode_req2] [small_prefill] [chunk]â”‚
â”‚          ^â”€ 1 token â”€^ ^â”€ 1 token â”€^ ^â”€â”€â”€ 50 tokens â”€â”€^ ^512^â”‚
â”‚                                                             â”‚
â”‚ Batch 2: [decode_req3] [decode_req4] [small_prefill] [chunk]â”‚
â”‚          ^â”€ 1 token â”€^ ^â”€ 1 token â”€^ ^â”€â”€â”€ 30 tokens â”€â”€^ ^512^â”‚
â”‚                                                             â”‚
â”‚ Result: Everyone gets served quickly!                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Priority System:
1. ğŸ”¥ Decode steps (existing conversations) - 1 token each
2. âš¡ Small prefill requests (new short conversations)
3. ğŸŒ One chunk from large prefill requests
```

### 3. GPU Utilization - From 45% to 92%

```
GPU Utilization Comparison:

Traditional Serving:
GPU â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    â–ˆâ–ˆâ–ˆâ–ˆ                    
    â”‚^â”€ big req â”€^                    ^sm^                    
    â”‚            idle time            req                     
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
    Utilization: 45% (lots of idle time)

Chunked Prefill Serving:
GPU â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â”‚^chunk^decode^chunk^decode^small^chunk^decode^small^...
    â”‚                                                        
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
    Utilization: 92% (always busy!)
```

## Real-World Performance Impact

```
Metrics Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    â”‚ Traditional â”‚ Chunked Prefill          â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Small Req Latency  â”‚ 8.2 seconds â”‚ 0.15 seconds âœ…         â”‚
â”‚ Throughput         â”‚ 15 req/min  â”‚ 85 req/min âœ…           â”‚
â”‚ GPU Utilization    â”‚ 45%         â”‚ 92% âœ…                  â”‚
â”‚ Memory Efficiency  â”‚ OOM errors  â”‚ Stable âœ…               â”‚
â”‚ Request Fairness   â”‚ Poor        â”‚ Excellent âœ…            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: 5.7x better throughput, 55x lower latency for small requests!
```

## How Modern Systems Use Chunked Prefill

### vLLM's Approach
```
vLLM Scheduling Strategy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Collect requests from queue                              â”‚
â”‚ 2. Fill batch with decode steps first (priority)           â”‚
â”‚ 3. Add small prefill requests                               â”‚
â”‚ 4. Add ONE chunk from large prefill request                â”‚
â”‚ 5. Process mixed batch                                      â”‚
â”‚ 6. Repeat                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Typical Batch (2048 tokens max):
- 8 decode steps (8 tokens)
- 3 small prefill requests (400 tokens)
- 1 large request chunk (512 tokens)
- Remaining space for more decode/small requests
```

### TensorRT-LLM's Approach
```
TensorRT-LLM Strategy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ - Adaptive chunk sizing based on available memory          â”‚
â”‚ - KV cache block management for efficient memory reuse     â”‚
â”‚ - Continuous batching with mixed request types             â”‚
â”‚ - Optimized CUDA kernels for chunk processing              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Configuration Guidelines

### Chunk Size Selection
```
Chunk Size Guidelines:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Size    â”‚ Recommended Chunk Size â”‚ Reasoning          â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Small (7B)    â”‚ 1024-2048 tokens      â”‚ More memory availableâ”‚
â”‚ Medium (13-30B)â”‚ 512-1024 tokens      â”‚ Balanced approach   â”‚
â”‚ Large (70B+)  â”‚ 256-512 tokens       â”‚ Memory constrained  â”‚
â”‚ Massive (175B+)â”‚ 128-256 tokens       â”‚ Very tight memory   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Memory Considerations:
- Available GPU memory
- Batch size requirements  
- Sequence length distribution
- Number of concurrent requests
```

### Performance Tuning
```
Optimization Factors:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Factor                â”‚ Impact                              â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Chunk Size            â”‚ Memory vs. scheduling flexibility   â”‚
â”‚ Batch Composition     â”‚ Decode/prefill ratio optimization  â”‚
â”‚ Request Prioritizationâ”‚ Latency vs. throughput tradeoffs   â”‚
â”‚ Memory Management     â”‚ KV cache efficiency                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Best Practices

### 1. Scheduling Strategy
- **Prioritize decode steps** - they're fast and keep conversations flowing
- **Batch small prefills together** - similar memory characteristics
- **Limit chunks per batch** - usually 1-2 chunks maximum
- **Monitor queue depths** - prevent starvation of large requests

### 2. Memory Management
- **Start conservative** with chunk sizes, then optimize
- **Monitor peak memory usage** during mixed batches
- **Use KV cache pooling** to reduce allocation overhead
- **Consider CPU offloading** for very long sequences

### 3. Request Fairness
- **Set maximum wait times** for large requests
- **Use weighted scheduling** based on request size
- **Implement backpressure** when queues get too long
- **Monitor latency percentiles** across request sizes

## Common Pitfalls

### âŒ What NOT to Do
- **Chunks too small**: Overhead dominates, poor GPU utilization
- **Chunks too large**: Memory spikes, defeats the purpose
- **Ignoring decode priority**: Existing conversations become slow
- **No request mixing**: Back to the original monopoly problem
- **Fixed chunk sizes**: Doesn't adapt to different workloads

### âœ… What TO Do
- **Adaptive chunk sizing** based on available memory
- **Smart batch composition** with mixed request types
- **Continuous monitoring** of latency and throughput metrics
- **Gradual optimization** starting from conservative settings

## The Bottom Line

Chunked prefill transforms LLM serving from a **"first-come, first-served"** system where large requests monopolize resources, into a **"fair time-sharing"** system where everyone gets served quickly.

**Key Insight**: It's not about reducing total computation - it's about enabling **request interleaving** and **memory management** that makes high-throughput, low-latency serving possible.

This is why modern systems like vLLM can serve 100x more requests per second than naive implementations!