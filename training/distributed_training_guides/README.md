# Distributed Training Guides

This directory contains comprehensive guides for distributed training using various frameworks:

## Guides Available

1. **PyTorch DDP** - Basic distributed data parallel training
2. **NVIDIA NeMo** - End-to-end framework for large models
3. **NCCL** - Low-level collective communications
4. **Megatron-LM** - Large-scale model parallel training

## Quick Start

Each guide includes:
- Setup instructions
- Code examples
- Launch scripts
- Performance tips
- Troubleshooting

## Hardware Requirements

- Multiple NVIDIA GPUs
- CUDA 11.8+
- InfiniBand/NVLink for multi-node (recommended)